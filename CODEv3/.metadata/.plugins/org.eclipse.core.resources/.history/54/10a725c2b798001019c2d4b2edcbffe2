#include "hal_data.h"
#include "eegTYPES.h"
#include "cognitiveSTATES.h"
#include "signalPROCESSING.h"
#include "semaphoresGLOBAL.h"

#include <math.h>
#include <string.h>
#include <stdlib.h>
#include <stdbool.h>

// ✅ μT-Kernel typedefs and constants
#ifndef INT
typedef int INT;
#endif
#ifndef ER
typedef int ER;
#endif
#ifndef ID
typedef int ID;
#endif
#ifndef UINT
typedef unsigned int UINT;
#endif
#ifndef TMO_FEVR
#define TMO_FEVR (-1)
#endif
#ifndef E_OK
#define E_OK (0)  // ✅ FIXED: Define E_OK
#endif

// ✅ Forward declarations for μT-Kernel functions
extern ER tk_wai_sem(ID semid, INT timeout);
extern ER tk_sig_sem(ID semid, INT cnt);  // ✅ FIXED: Added declaration
extern ER tk_dly_tsk(INT dlytim);         // ✅ FIXED: Added declaration
// ✅ ADD: Missing external function declarations
extern fsp_err_t signal_processing_get_buffer(float **left_buffer,
                                             float **right_buffer,
                                             uint32_t *buffer_size);
extern fsp_err_t trigger_drowsiness_alert(void);
extern fsp_err_t trigger_haptic_pattern(cognitive_state_type_t state);

/* DSP and ML Constants */
#define FFT_SIZE 256
#define FFT_SIZE_HALF 128
#define SAMPLE_RATE 500.0f
#define FREQ_RESOLUTION (SAMPLE_RATE / FFT_SIZE)
#define PI 3.14159265358979323846f

/* EEG Frequency Band Definitions */
#define DELTA_START_BIN (int)(0.5f / FREQ_RESOLUTION)
#define DELTA_END_BIN (int)(4.0f / FREQ_RESOLUTION)
#define THETA_START_BIN (int)(4.0f / FREQ_RESOLUTION)
#define THETA_END_BIN (int)(8.0f / FREQ_RESOLUTION)
#define ALPHA_START_BIN (int)(8.0f / FREQ_RESOLUTION)
#define ALPHA_END_BIN (int)(13.0f / FREQ_RESOLUTION)
#define BETA_START_BIN (int)(13.0f / FREQ_RESOLUTION)
#define BETA_END_BIN (int)(30.0f / FREQ_RESOLUTION)
#define GAMMA_START_BIN (int)(30.0f / FREQ_RESOLUTION)
#define GAMMA_END_BIN (int)(45.0f / FREQ_RESOLUTION)
#define STRESS_THRESHOLD 0.7f
#define FATIGUE_THRESHOLD 0.8f
#define ANXIETY_THRESHOLD 0.75f
/* Neural Network Architecture */
#define INPUT_FEATURES 24
#define HIDDEN_LAYER1_SIZE 16
#define HIDDEN_LAYER2_SIZE 12
#define OUTPUT_LAYER_SIZE 6

/* Feature Extraction Structures */
typedef struct {
    float real;
    float imag;
} complex_t;

typedef struct {
    float magnitude[FFT_SIZE_HALF];
    float phase[FFT_SIZE_HALF];
    float power_spectrum[FFT_SIZE_HALF];
} fft_result_t;

/* Lightweight Neural Network Weights (Pre-trained) */
typedef struct {
    float w1[INPUT_FEATURES][HIDDEN_LAYER1_SIZE];
    float b1[HIDDEN_LAYER1_SIZE];
    float w2[HIDDEN_LAYER1_SIZE][HIDDEN_LAYER2_SIZE];
    float b2[HIDDEN_LAYER2_SIZE];
    float w3[HIDDEN_LAYER2_SIZE][OUTPUT_LAYER_SIZE];
    float b3[OUTPUT_LAYER_SIZE];
} neural_network_t;

/* Global Variables */
feature_vector_t current_features;
static cognitive_classification_t classification_result;
static neural_network_t cognitive_nn;
static volatile bool classifier_initialized = false;
static volatile uint32_t classifications_performed = 0;

/* External semaphore references */
extern ID feature_extraction_semaphore;
extern ID classification_semaphore;
extern ID haptic_semaphore;
extern ID communication_semaphore;

/* Private Function Prototypes */
static void init_neural_network(void);
static void compute_fft(const float *input, complex_t *output, int size);
void extract_frequency_features(const float *left_signal, const float *right_signal, int size);
void extract_time_domain_features(const float *left_signal, const float *right_signal, int size);
void extract_coherence_features(const float *left_signal, const float *right_signal, int size);
void extract_quality_features(const float *left_signal, const float *right_signal, int size);
static float calculate_spectral_entropy(const float *power_spectrum, int size);
static float calculate_hjorth_parameters(const float *signal, int size, float *mobility);
static float sigmoid_activation(float x);
static float relu_activation(float x);
static void forward_propagation(const feature_vector_t *features, float *output);
static cognitive_state_type_t determine_dominant_state(const float *probabilities);
static bool intervention_required(const cognitive_classification_t *result);

// ✅ Public function declarations (add to header later)
void task_feature_extraction_entry(INT stacd, void *exinf);
void task_classification_entry(INT stacd, void *exinf);

/**
 * @brief Initialize cognitive classification system
 */
fsp_err_t cognitive_classifier_init(void)
{
    memset(&current_features, 0, sizeof(current_features));
    memset(&classification_result, 0, sizeof(classification_result));

    init_neural_network();

    classifications_performed = 0;
    classifier_initialized = true;

    return FSP_SUCCESS;
}

/**
 * @brief Initialize pre-trained neural network weights
 */
static void init_neural_network(void)
{
    /* Input to Hidden Layer 1 */
    for (int i = 0; i < INPUT_FEATURES; i++) {
        for (int j = 0; j < HIDDEN_LAYER1_SIZE; j++) {
            cognitive_nn.w1[i][j] = ((float)rand() / (float)RAND_MAX - 0.5f) * 0.2f; // ✅ FIXED cast
        }
    }

    for (int i = 0; i < HIDDEN_LAYER1_SIZE; i++) {
        cognitive_nn.b1[i] = 0.0f;
    }

    /* Hidden Layer 1 to Hidden Layer 2 */
    for (int i = 0; i < HIDDEN_LAYER1_SIZE; i++) {
        for (int j = 0; j < HIDDEN_LAYER2_SIZE; j++) {
            cognitive_nn.w2[i][j] = ((float)rand() / (float)RAND_MAX - 0.5f) * 0.2f; // ✅ FIXED cast
        }
    }

    for (int i = 0; i < HIDDEN_LAYER2_SIZE; i++) {
        cognitive_nn.b2[i] = 0.0f;
    }

    /* Hidden Layer 2 to Output */
    for (int i = 0; i < HIDDEN_LAYER2_SIZE; i++) {
        for (int j = 0; j < OUTPUT_LAYER_SIZE; j++) {
            cognitive_nn.w3[i][j] = ((float)rand() / (float)RAND_MAX - 0.5f) * 0.2f; // ✅ FIXED cast
        }
    }

    for (int i = 0; i < OUTPUT_LAYER_SIZE; i++) {
        cognitive_nn.b3[i] = 0.0f;
    }
}

/**
 * @brief Compute FFT using simple DFT
 */
static void compute_fft(const float *input, complex_t *output, int size)
{
    const float pi2 = 2.0f * PI;

    for (int k = 0; k < size/2; k++) {
        output[k].real = 0.0f;
        output[k].imag = 0.0f;

        for (int n = 0; n < size; n++) {
            float angle = -pi2 * (float)k * (float)n / (float)size; // ✅ FIXED casts
            output[k].real += input[n] * cosf(angle);
            output[k].imag += input[n] * sinf(angle);
        }

        output[k].real /= (float)size; // ✅ FIXED cast
        output[k].imag /= (float)size; // ✅ FIXED cast
    }
}

/**
 * @brief Extract frequency domain features
 */
void extract_frequency_features(const float *left_signal, const float *right_signal, int size)
{
    complex_t left_fft[FFT_SIZE_HALF];
    complex_t right_fft[FFT_SIZE_HALF];
    float combined_power[FFT_SIZE_HALF];

    compute_fft(left_signal, left_fft, size);
    compute_fft(right_signal, right_fft, size);

    for (int i = 0; i < FFT_SIZE_HALF; i++) {
        float left_power = left_fft[i].real * left_fft[i].real + left_fft[i].imag * left_fft[i].imag;
        float right_power = right_fft[i].real * right_fft[i].real + right_fft[i].imag * right_fft[i].imag;
        combined_power[i] = (left_power + right_power) / 2.0f;
    }

    /* Extract frequency band powers */
    current_features.delta_power = 0.0f;
    for (int i = DELTA_START_BIN; i <= DELTA_END_BIN && i < FFT_SIZE_HALF; i++) {
        current_features.delta_power += combined_power[i];
    }

    current_features.theta_power = 0.0f;
    for (int i = THETA_START_BIN; i <= THETA_END_BIN && i < FFT_SIZE_HALF; i++) {
        current_features.theta_power += combined_power[i];
    }

    current_features.alpha_power = 0.0f;
    for (int i = ALPHA_START_BIN; i <= ALPHA_END_BIN && i < FFT_SIZE_HALF; i++) {
        current_features.alpha_power += combined_power[i];
    }

    current_features.beta_power = 0.0f;
    for (int i = BETA_START_BIN; i <= BETA_END_BIN && i < FFT_SIZE_HALF; i++) {
        current_features.beta_power += combined_power[i];
    }

    current_features.gamma_power = 0.0f;
    for (int i = GAMMA_START_BIN; i <= GAMMA_END_BIN && i < FFT_SIZE_HALF; i++) {
        current_features.gamma_power += combined_power[i];
    }

    /* Calculate band ratios */
    current_features.alpha_beta_ratio = (current_features.beta_power > 0) ?
        current_features.alpha_power / current_features.beta_power : 0.0f;
    current_features.theta_alpha_ratio = (current_features.alpha_power > 0) ?
        current_features.theta_power / current_features.alpha_power : 0.0f;

    /* Calculate spectral entropy */
    current_features.spectral_entropy = calculate_spectral_entropy(combined_power, FFT_SIZE_HALF);

    /* Find peak frequency */
    int peak_bin = 0;
    float max_power = combined_power[0];
    for (int i = 1; i < FFT_SIZE_HALF; i++) {
        if (combined_power[i] > max_power) {
            max_power = combined_power[i];
            peak_bin = i;
        }
    }
    current_features.peak_frequency = (float)peak_bin * FREQ_RESOLUTION; // ✅ FIXED cast

    /* Calculate spectral centroid */
    float numerator = 0.0f, denominator = 0.0f;
    for (int i = 0; i < FFT_SIZE_HALF; i++) {
        float frequency = (float)i * FREQ_RESOLUTION; // ✅ FIXED cast
        numerator += frequency * combined_power[i];
        denominator += combined_power[i];
    }
    current_features.spectral_centroid = (denominator > 0) ? numerator / denominator : 0.0f;
}

/**
 * @brief Extract time domain features
 */
void extract_time_domain_features(const float *left_signal, const float *right_signal, int size)
{
    float combined_signal[FFT_SIZE];
    for (int i = 0; i < size; i++) {
        combined_signal[i] = (left_signal[i] + right_signal[i]) / 2.0f;
    }

    /* Calculate mean */
    float sum = 0.0f;
    for (int i = 0; i < size; i++) {
        sum += combined_signal[i];
    }
    current_features.mean_amplitude = sum / (float)size; // ✅ FIXED cast

    /* Calculate RMS */
    float sum_squares = 0.0f;
    for (int i = 0; i < size; i++) {
        sum_squares += combined_signal[i] * combined_signal[i];
    }
    current_features.rms_amplitude = sqrtf(sum_squares / (float)size); // ✅ FIXED cast

    /* Calculate variance */
    float variance_sum = 0.0f;
    for (int i = 0; i < size; i++) {
        float diff = combined_signal[i] - current_features.mean_amplitude;
        variance_sum += diff * diff;
    }
    current_features.variance = variance_sum / (float)(size - 1); // ✅ FIXED cast

    /* Calculate skewness and kurtosis */
    float std_dev = sqrtf(current_features.variance);
    float skew_sum = 0.0f, kurt_sum = 0.0f;

    if (std_dev > 0) {
        for (int i = 0; i < size; i++) {
            float normalized = (combined_signal[i] - current_features.mean_amplitude) / std_dev;
            float normalized_cubed = normalized * normalized * normalized;
            float normalized_fourth = normalized_cubed * normalized;
            skew_sum += normalized_cubed;
            kurt_sum += normalized_fourth;
        }
        current_features.skewness = skew_sum / (float)size; // ✅ FIXED cast
        current_features.kurtosis = kurt_sum / (float)size - 3.0f; // ✅ FIXED cast
    } else {
        current_features.skewness = 0.0f;
        current_features.kurtosis = 0.0f;
    }

    /* Calculate zero crossing rate */
    int zero_crossings = 0;
    for (int i = 1; i < size; i++) {
        if ((combined_signal[i] >= 0 && combined_signal[i-1] < 0) ||
            (combined_signal[i] < 0 && combined_signal[i-1] >= 0)) {
            zero_crossings++;
        }
    }
    current_features.zero_crossing_rate = (float)zero_crossings / (float)(size - 1); // ✅ FIXED cast

    /* Calculate Hjorth parameters */
    current_features.hjorth_activity = current_features.variance;
    current_features.hjorth_mobility = calculate_hjorth_parameters(combined_signal, size, &current_features.hjorth_mobility);
}

/**
 * @brief Calculate Hjorth mobility parameter
 */
static float calculate_hjorth_parameters(const float *signal, int size, float *mobility)
{
    float derivative[FFT_SIZE-1];
    for (int i = 0; i < size-1; i++) {
        derivative[i] = signal[i+1] - signal[i];
    }

    /* Calculate variance of derivative */
    float deriv_mean = 0.0f;
    for (int i = 0; i < size-1; i++) {
        deriv_mean += derivative[i];
    }
    deriv_mean /= (float)(size-1); // ✅ FIXED cast

    float deriv_variance = 0.0f;
    for (int i = 0; i < size-1; i++) {
        float diff = derivative[i] - deriv_mean;
        deriv_variance += diff * diff;
    }
    deriv_variance /= (float)(size-2); // ✅ FIXED cast

    *mobility = (current_features.hjorth_activity > 0) ?
        sqrtf(deriv_variance / current_features.hjorth_activity) : 0.0f;

    return *mobility;
}

/**
 * @brief Extract coherence features between channels
 */
void extract_coherence_features(const float *left_signal, const float *right_signal, int size)
{
    /* Calculate cross-correlation */
    float correlation_sum = 0.0f, left_sum = 0.0f, right_sum = 0.0f;
    for (int i = 0; i < size; i++) {
        correlation_sum += left_signal[i] * right_signal[i];
        left_sum += left_signal[i] * left_signal[i];
        right_sum += right_signal[i] * right_signal[i];
    }

    float denominator = sqrtf(left_sum * right_sum);
    current_features.cross_correlation = (denominator > 0) ? correlation_sum / denominator : 0.0f;

    /* Simplified coherence calculation */
    current_features.coherence_alpha = fabsf(current_features.cross_correlation) *
        (current_features.alpha_power / (current_features.alpha_power + current_features.beta_power + 1e-6f));
    current_features.coherence_beta = fabsf(current_features.cross_correlation) *
        (current_features.beta_power / (current_features.alpha_power + current_features.beta_power + 1e-6f));

    current_features.phase_lag_index = (1.0f - fabsf(current_features.cross_correlation)) / 2.0f;
}

/**
 * @brief Extract signal quality features
 */
void extract_quality_features(const float *left_signal, const float *right_signal, int size)
{
    (void)left_signal;   // ✅ Suppress unused parameter warnings
    (void)right_signal;
    (void)size;

    float signal_power = current_features.rms_amplitude * current_features.rms_amplitude;
    float noise_estimate = current_features.variance * 0.1f;
    current_features.snr_estimate = (noise_estimate > 0) ?
        10.0f * log10f(signal_power / noise_estimate) : 0.0f;

    current_features.signal_stability = 1.0f / (1.0f + current_features.variance);
}

/**
 * @brief Calculate spectral entropy
 */
static float calculate_spectral_entropy(const float *power_spectrum, int size)
{
    float total_power = 0.0f;
    float entropy = 0.0f;

    for (int i = 0; i < size; i++) {
        total_power += power_spectrum[i];
    }

    if (total_power <= 0) return 0.0f;

    for (int i = 0; i < size; i++) {
        if (power_spectrum[i] > 0) {
            float probability = power_spectrum[i] / total_power;
            entropy -= probability * log2f(probability);
        }
    }

    float max_entropy = log2f((float)size);
    return (max_entropy > 0) ? entropy / max_entropy : 0.0f;
}

/**
 * @brief Activation functions
 */
static float sigmoid_activation(float x)
{
    return 1.0f / (1.0f + expf(-x));
}

static float relu_activation(float x)
{
    return (x > 0) ? x : 0.0f;
}

/**
 * @brief Forward propagation through neural network
 */
static void forward_propagation(const feature_vector_t *features, float *output)
{
    float input[INPUT_FEATURES] = {
        features->delta_power, features->theta_power, features->alpha_power,
        features->beta_power, features->gamma_power, features->alpha_beta_ratio,
        features->theta_alpha_ratio, features->spectral_entropy, features->peak_frequency,
        features->spectral_centroid, features->mean_amplitude, features->rms_amplitude,
        features->variance, features->skewness, features->kurtosis, features->zero_crossing_rate,
        features->hjorth_activity, features->hjorth_mobility, features->cross_correlation,
        features->coherence_alpha, features->coherence_beta, features->phase_lag_index,
        features->snr_estimate, features->signal_stability
    };

    /* Hidden Layer 1 */
    float hidden1[HIDDEN_LAYER1_SIZE];
    for (int i = 0; i < HIDDEN_LAYER1_SIZE; i++) {
        float sum = cognitive_nn.b1[i];
        for (int j = 0; j < INPUT_FEATURES; j++) {
            sum += input[j] * cognitive_nn.w1[j][i];
        }
        hidden1[i] = relu_activation(sum);
    }

    /* Hidden Layer 2 */
    float hidden2[HIDDEN_LAYER2_SIZE];
    for (int i = 0; i < HIDDEN_LAYER2_SIZE; i++) {
        float sum = cognitive_nn.b2[i];
        for (int j = 0; j < HIDDEN_LAYER1_SIZE; j++) {
            sum += hidden1[j] * cognitive_nn.w2[j][i];
        }
        hidden2[i] = relu_activation(sum);
    }

    /* Output Layer */
    float raw_output[OUTPUT_LAYER_SIZE];
    for (int i = 0; i < OUTPUT_LAYER_SIZE; i++) {
        float sum = cognitive_nn.b3[i];
        for (int j = 0; j < HIDDEN_LAYER2_SIZE; j++) {
            sum += hidden2[j] * cognitive_nn.w3[j][i];
        }
        raw_output[i] = sum;
    }

    /* Apply softmax activation */
    float exp_sum = 0.0f;
    for (int i = 0; i < OUTPUT_LAYER_SIZE; i++) {
        output[i] = expf(raw_output[i]);
        exp_sum += output[i];
    }

    if (exp_sum > 0) {
        for (int i = 0; i < OUTPUT_LAYER_SIZE; i++) {
            output[i] /= exp_sum;
        }
    }
}

/**
 * @brief Determine dominant cognitive state
 */
static cognitive_state_type_t determine_dominant_state(const float *probabilities)
{
    int max_index = 0;
    float max_prob = probabilities[0];

    for (int i = 1; i < OUTPUT_LAYER_SIZE; i++) {
        if (probabilities[i] > max_prob) {
            max_prob = probabilities[i];
            max_index = i;
        }
    }

    return (cognitive_state_type_t)max_index;
}

/**
 * @brief Check if intervention is required
 */
static bool intervention_required(const cognitive_classification_t *result)
{
    if (result->dominant_state == COGNITIVE_STATE_STRESS &&
        result->confidence_scores[COGNITIVE_STATE_STRESS] > 0.7f) return true;
    if (result->dominant_state == COGNITIVE_STATE_ANXIETY &&
        result->confidence_scores[COGNITIVE_STATE_ANXIETY] > 0.75f) return true;
    if (result->dominant_state == COGNITIVE_STATE_FATIGUE &&
        result->confidence_scores[COGNITIVE_STATE_FATIGUE] > 0.8f) return true;

    return false;
}

/**
 * @brief μT-Kernel Task: Feature Extraction (5Hz)
 */
void task_feature_extraction_entry(INT stacd, void *exinf)
{
    (void)stacd;
    (void)exinf;

    float *left_buffer, *right_buffer;
    uint32_t buffer_size;
    ER ercd;

    printf("SHRAVYA: ✅ Feature extraction task ready\r\n");

    while(1)
    {
        // ✅ NEW: Add debug before waiting
        printf("SHRAVYA: 🧠 Waiting for feature extraction semaphore...\r\n");

        ercd = tk_wai_sem(feature_extraction_semaphore, TMO_FEVR);
        if (ercd != E_OK) {
            // ✅ NEW: Add debug for semaphore failure
            printf("SHRAVYA: ❌ Feature extraction semaphore failed: %d\r\n", ercd);
            continue; // ✅ EXISTING: E_OK now defined
        }

        printf("SHRAVYA: 🧠 Feature Extraction - Starting...\r\n");

        if (signal_processing_get_buffer(&left_buffer, &right_buffer, &buffer_size) == FSP_SUCCESS)
        {
            // ✅ NEW: Add debug for buffer size
            printf("SHRAVYA: 📊 Processing %u samples from signal buffer\r\n", buffer_size);

            /* ✅ EXISTING: Cast uint32_t to int to avoid warnings */
            extract_frequency_features(left_buffer, right_buffer, (int)buffer_size);

            // ✅ NEW: Add debug after frequency features
            printf("SHRAVYA: ⚡ Frequency features extracted\r\n");

            extract_time_domain_features(left_buffer, right_buffer, (int)buffer_size);

            // ✅ NEW: Add debug after time domain features
            printf("SHRAVYA: ⏱️ Time domain features extracted\r\n");

            extract_coherence_features(left_buffer, right_buffer, (int)buffer_size);

            // ✅ NEW: Add debug after coherence features
            printf("SHRAVYA: 🔗 Coherence features extracted\r\n");

            extract_quality_features(left_buffer, right_buffer, (int)buffer_size);

            // ✅ NEW: Add debug after quality features
            printf("SHRAVYA: 🎛️ Quality features extracted\r\n");

            // ✅ EXISTING: Add safety check for division by zero
            float focus_score = (current_features.beta_power > 0.000001f) ?
                               (current_features.alpha_power / current_features.beta_power) : 0.0f;

            printf("SHRAVYA: ✅ Features extracted - Alpha: %.3f, Beta: %.3f, Focus Score: %.2f\r\n",
                   current_features.alpha_power, current_features.beta_power, focus_score);

            // ✅ NEW: Add additional feature details
            printf("SHRAVYA: 📈 Time features - RMS: %.3f, Variance: %.3f, ZCR: %.3f\r\n",
                   current_features.rms_amplitude, current_features.variance, current_features.zero_crossing_rate);

            // ✅ NEW: Add coherence details
            printf("SHRAVYA: 🧠 Coherence - Cross-corr: %.3f, Alpha-coh: %.3f, Beta-coh: %.3f\r\n",
                   current_features.cross_correlation, current_features.coherence_alpha, current_features.coherence_beta);

            tk_sig_sem(classification_semaphore, 1);
            printf("SHRAVYA: 📊 Signaling cognitive classification...\r\n");

            /* WAIT for classification to complete */
            printf("SHRAVYA: ⏳ Waiting for classification to complete...\r\n");
            ER class_wait = tk_wai_sem(classification_ready_semaphore, TMO_FEVR);
            if (class_wait == E_OK) {
                printf("SHRAVYA: ✅ Classification completed\r\n");
            } else {
                printf("SHRAVYA: ⚠️ Classification wait failed: %d\r\n", class_wait);
            }

            /* Signal back that features are done */
            tk_sig_sem(features_ready_semaphore, 1);
            printf("SHRAVYA: ✅ Feature extraction cycle complete\r\n");

        }
        else
        {
            // ✅ EXISTING: Debug log when buffer not available
            printf("SHRAVYA: ⚠️ Signal processing buffer not ready\r\n");
        }
    }
}


/**
 * @brief μT-Kernel Task: Cognitive Classification (5Hz)
 */
void task_classification_entry(INT stacd, void *exinf)
{
    (void)stacd;
    (void)exinf;

    ER ercd;
    uint32_t start_time, end_time;

    if (cognitive_classifier_init() != FSP_SUCCESS)
    {
        printf("SHRAVYA: ❌ Cognitive classifier initialization failed\r\n");
        while(1) tk_dly_tsk(1000); // ✅ FIXED: Now declared
    }

    printf("SHRAVYA: ✅ Cognitive classifier task ready\r\n");

    while(1)
    {
        ercd = tk_wai_sem(classification_semaphore, TMO_FEVR);
        if (ercd != E_OK) continue; // ✅ FIXED: E_OK now defined

        printf("SHRAVYA: 🤖 AI Classification - Running neural network...\r\n");

        /* ✅ FIXED: Provide required clock parameter */
        start_time = R_FSP_SystemClockHzGet(FSP_PRIV_CLOCK_ICLK) / 1000;

        forward_propagation(&current_features, classification_result.confidence_scores);
        classification_result.dominant_state = determine_dominant_state(classification_result.confidence_scores);

        classification_result.overall_wellness_score =
            classification_result.confidence_scores[COGNITIVE_STATE_CALM] * 0.4f +
            classification_result.confidence_scores[COGNITIVE_STATE_FOCUS] * 0.3f +
            (1.0f - classification_result.confidence_scores[COGNITIVE_STATE_STRESS]) * 0.2f +
            (1.0f - classification_result.confidence_scores[COGNITIVE_STATE_ANXIETY]) * 0.1f;

        printf("SHRAVYA: 🎯 AI Result - State: %d, Focus: %.2f, Stress: %.2f, Wellness: %.2f\r\n",
               classification_result.dominant_state,
               classification_result.confidence_scores[COGNITIVE_STATE_FOCUS],
               classification_result.confidence_scores[COGNITIVE_STATE_STRESS],
               classification_result.overall_wellness_score);

        classification_result.intervention_needed = intervention_required(&classification_result);

        /* ✅ FIXED: Provide required clock parameter */
        end_time = R_FSP_SystemClockHzGet(FSP_PRIV_CLOCK_ICLK) / 1000;
        classification_result.inference_time_ms = end_time - start_time;

        // ✅ FIXED: Only increment once
        classifications_performed++;

        // NEW: Enhanced intervention handling with producer-consumer pattern
        if (classification_result.intervention_needed)
        {
            printf("SHRAVYA: ⚠️ Intervention needed - triggering haptic feedback\r\n");
            tk_sig_sem(haptic_semaphore, 1);
            tk_sig_sem(communication_semaphore, 1);  // Also trigger N8N communication

            /* WAIT for feedback to complete */
            printf("SHRAVYA: ⏳ Waiting for feedback to complete...\r\n");
            ER feedback_wait = tk_wai_sem(feedback_ready_semaphore, TMO_FEVR);
            if (feedback_wait == E_OK) {
                printf("SHRAVYA: ✅ Feedback completed\r\n");
            } else {
                printf("SHRAVYA: ⚠️ Feedback wait failed: %d\r\n", feedback_wait);
            }
        } else {
            printf("SHRAVYA: ✅ No intervention needed - classification complete\r\n");
            // Even if no intervention, signal feedback ready to maintain flow
            tk_sig_sem(feedback_ready_semaphore, 1);
        }

        // ✅ FIXED: Debug log every 10th classification
        if (classifications_performed % 10 == 0) {
            printf("SHRAVYA: 📈 Total classifications: %u, Avg wellness: %.2f\r\n",
                   classifications_performed, classification_result.overall_wellness_score);
        }

        /* NEW: Signal back that classification is done */
        tk_sig_sem(classification_ready_semaphore, 1);
        printf("SHRAVYA: ✅ AI classification cycle complete\r\n");

    } // ✅ FIXED: Added missing closing brace
}

/**
 * @brief Get latest classification result
 */
fsp_err_t get_classification_result(cognitive_classification_t *result)
{
    if (!result || !classifier_initialized) return FSP_ERR_INVALID_POINTER;

    *result = classification_result;
    return FSP_SUCCESS;
}

/**
 * @brief Get current feature vector
 */
fsp_err_t get_feature_vector(feature_vector_t *features)
{
    if (!features) return FSP_ERR_INVALID_POINTER;

    *features = current_features;
    return FSP_SUCCESS;
}

/**
 * ✅ ADD: Trigger specific interventions based on cognitive state
 */
/**
 * ✅ FIXED: Trigger specific interventions based on cognitive state
 */
void handle_intervention_trigger(cognitive_classification_t *result)
{
    switch(result->dominant_state) {
        case COGNITIVE_STATE_FATIGUE:
            if (result->confidence_scores[COGNITIVE_STATE_FATIGUE] > FATIGUE_THRESHOLD) {
                trigger_drowsiness_alert(); // Wake up drowsy user
            }
            break;

        case COGNITIVE_STATE_STRESS:
            if (result->confidence_scores[COGNITIVE_STATE_STRESS] > STRESS_THRESHOLD) {
                trigger_haptic_pattern(COGNITIVE_STATE_STRESS); // Stress relief
            }
            break;

        case COGNITIVE_STATE_ANXIETY:
            if (result->confidence_scores[COGNITIVE_STATE_ANXIETY] > ANXIETY_THRESHOLD) {
                trigger_haptic_pattern(COGNITIVE_STATE_ANXIETY); // Calming pattern
            }
            break;

        default:
            // No intervention needed
            break;
    }
} // ✅ FIXED: Added missing closing brace

/**
 * ✅ FIXED: Get classification statistics
 */
fsp_err_t get_classification_statistics(uint32_t *total_classifications,
                                       float *avg_wellness,
                                       bool *is_active)
{
    if (!total_classifications || !avg_wellness || !is_active) {
        return FSP_ERR_INVALID_POINTER;
    }

    *total_classifications = classifications_performed;
    *avg_wellness = classification_result.overall_wellness_score;
    *is_active = classifier_initialized;

    return FSP_SUCCESS;
}
